## 神经网络和深度学习 ##

### Note1 ###

**单个神经元**

**传递函数**：

常用的：阶梯函数（step），符号函数（sgn），线性函数（Linear），饱和线性函数（Ramp），对数S型函数（Sigmoid），双曲正切S型函数（Tanh）。

**感知机**：

感知机是最简单的神经网络。

>Neuroph:基于Java的神经网络框架

感知机无法解决XOR问题（可以解决AND和OR问题）。

>线性不可分是导致不能解决XOR问题的原因，而AND和OR则是线性可分的问题

**多层神经网络**

XOR问题可以通过在单层神经网络多加一层，并且利用Back-Propagating（BP）算法来解决。

1、感知器

双层神经网络模型，输入层和计算单元。

2、多层神经网络（前馈神经网络）

3、计算单元 

任意输入，单一输出。

4、层

### Note2 ###

  - 监督学习
  - 非监督学习
  - 强化学习（Reinforcement Learning）
  回报函数，无label，本质是马尔科夫决策过程，最终目的是决策过程中整体回报函数期望最优	。

深度学习是神经网络的一大分支，深度学习的基本结构是深度神经网络。

**特征**

人类视觉系统提取的过程是由：Retina->V1->V2->V4，高级表达是由低级表达组成的。

1、特征粒度

2、浅层特征

复杂图形是由基本图形构件组成。类似基

3、结构性特征

Retina->像素(pixls)

V1->边缘(edges)

V2->构件(object parts)

V4->物体(object models)

**浅层学习和深层学习**

浅层学习：多层感知机，SVM，Boosting，Logistics Regression
。单个隐层或没有隐层。


深层学习：多个隐层。Hinton提出的（Deep Belief Network）DBN
划时代。其中重要的几个原则

>  - 非监督学习被用来预训练各个层
>  - 非监督学习在之前学习到的层次之上，一次只学习一个层次，每个层次学习的结果作为下一个层次的输入
>  - 用监督学习来调整层之间的权重。

**深度学习和神经网络**

>BP算法：神经网络的训练

>    - 信号正向传播
>    - 误差反向传播

问题在于梯度稀疏，误差校正越来越小从顶至下；收敛局部最小值。

 
### Note3 ###

**深度学习常用方法**

自编码器

有监督微调：只调整最后的分类器；调整整个系统。

栈式自编码器：提供预训练方法初始化网络权重。

限制玻尔兹曼机

1、生成模型和概率模型

   - 决策函数\\(Y=f(X)\\)和条件概率分布\\(P(Y/X)\\)可以互相转化。决策函数在分类器设计时就需要估计根据给定的训练数据进行估计概率模型。但是概率分布模型往往难以估计，所以我们转而从训练样本和问题出发来求出判别函数，也就是决策函数。这就包括了SVM是求出分类面，近邻法是求出预测数据通训练数据比较，取最近相同分类。


   - 生成方法(Generative Approach)和判别方法(Discriminative Approach),生成模型可以推出判别模型，反之不然。生成可以找到数据分布情况，反应同类数据本身相似度，但求不出边界。而判别式可以求出最有分类面。生成学习\\(p(x,y)\\),判别学习\\(p(y|x)\\)。\\(p(y|x)=p(x,y)/p(x)\\)

   - 能量模型。将求稳态转换成求极值优化问题。任何概率分布都可以转化成基于能量模型。系统有序或者概率分布越集中，系统能量越小，反之越大。能量模型提出就是求系统的稳态，也就是能量最小值。

2、RBM

RBM包括可见层，隐层和偏置层。可见层和隐层链接方向不确定，可以双向传播，完全连接的。

RBM几个参数有权重矩阵，可视层和隐层分别的偏置量。

用来：降维；初始化神经网络初始值；求联合概率；求条件概率。

3、DBN（Deep Belief Network）深度信念网络

RBM堆叠起来的DBN。用逐层贪婪训练方法（Greedy Layer-wise Training）。非监督单层，BP网络监督训练。

4、CNN卷积神经网络

结构：Convolution Layer 和 Subsample Layer（Pooling Layer）。

权值共享和参数减少。

5、RNN（循环神经网络），非递归神经网络，也叫RNN

LSTM典型的循环神经网络。



